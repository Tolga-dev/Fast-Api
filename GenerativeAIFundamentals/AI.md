# Generative AI Fundamentals

## Generative AI

### Introduction

* ANI
  * artificial narrow intelligence,
    * weak ai
    * cant make independent decisions
    * learn new skills
    * developer deep understanding world
    * preprogrammed algos
    * data and requires muhan
* AGI
  * artificial general intelligence,
    * general, strong
    * can understand, appply
* ASI
  * artificial super intelligence
    * super intelligence

### Key
* ai
  * unbrella term 
* ml
  * learn from experience
  * improve skill
  * analyze data
  * detect pattern
  * make prediction
  * decision
* dl
  * multi layered ai
  * object detection
  * speech recognition
  * language translation
  * dl analze data
  * image sound text
* nlp
* generative ai
  * ml
  * with dl
  * maasive datasets
  * identify patterns
* lm, language model
  * type of gen ai
### Types of language models
* lm
  * sophisticated ai
  * llm
  * sml
  * Use transformer architecture; 
  * Are pre-trained on massive volumes of text data.
  * 
### Tokens
* # Generative AI Fundamentals

## Generative AI

### Introduction

* ANI
  * artificial narrow intelligence,
    * weak ai
    * cant make independent decisions
    * learn new skills
    * developer deep understanding world
    * preprogrammed algos
    * data and requires muhan
* AGI
  * artificial general intelligence,
    * general, strong
    * can understand, appply
* ASI
  * artificial super intelligence
    * super intelligence

### Key
* ai
  * unbrella term 
* ml
  * learn from experience
  * improve skill
  * analyze data
  * detect pattern
  * make prediction
  * decision
* dl
  * multi layered ai
  * object detection
  * speech recognition
  * language translation
  * dl analze data
  * image sound text
* nlp
* generative ai
  * ml
  * with dl
  * maasive datasets
  * identify patterns
* lm, language model
  * type of gen ai
### Types of language models
* lm
  * sophisticated ai
  * llm
  * sml
  * Use transformer architecture; 
  * Are pre-trained on massive volumes of text data.
  * 
### Tokens
* llm break down prompts into tiny building blocks called tokens
  * token
    * single, unit of meaning in natural language
    * a whole word
    * smallest segment of a text 
    * bits of data
  * toneization
    * process of breaking down text data into tokens following certain predeterminedrules 
    * decoding human lang
    * make text data coppatible 
    * individual work, word toekn
    * parts of words, subword tokens
    * machine readable bytes, byte level token
    * individual char, char token
    * sentences, sentence token
    * other types
  * most used is word tokenization
  * max input, centain number of tokens in a single request
  * optimizing cost, number of token used

### LLM work
* transformer architecture
  * before transformers
    * it was sequential
    * could miss dependencies throughout long text sequences
    * due to self attention mechanism
  * parallel processing 
    * helps us more context aware interpretation of the language
    * speeds up training 
    * improve performance
  * it has encode and decoder
    * encoder, understanding context and decoder generating for output
    * it has multiple layers of neural networks
  * converting textual input into numerical data, because inner computations, based on math function
  * Arch
    * input
      * prompt, tokenized, text into smaller units, tokens, it has id based on model vocabulary
        * it is a list, each unique token from the training dataset, with uniqeue index
    * embedding
      * need to be converted into set of numbers, vectors, that define each tokens initial semantic meaning
      * this process, employs neural networks
    * positional encoding
      * special vectors are added directly to embeddings, same dimension as embeddings and help to preserve sequence info
      * even if same word appears, in different positions
    * self attention mech
      * enriched embeddings, they progress through the models levels
      * model in calculating attention scores, for entire input sequence
      * indicates how much focus should be placed on different words in sentence when trying to understand
      * model shifted meaninf of the cat, close to anumal, instand of different acronyms
    * decoder

### Prompt
* set of instructions or question
* defines task, for llm

### Cognitive Contraints and Interaction 
* non human
* hallucinations
* biased outputs
* lack of knowledge
* stochastic nature

### Retrieval-Augmented Generation (RAG) 
* retriever
* generator

* Identify and explain key terms associated with AI, including machine learning, deep learning, natural language processing, generative AI, and language models.
* Explain the tokenization process.
* Describe the transformers' architecture.
* Explain the prompt processing based on the transformers' architecture.
* Classify the limitations of LLM usage and give examples for each limitation type.
* Identify the reasons behind these limitations.
* Recognize security limitations.
* List mitigation strategies for cognitive and security limitations.
* Mitigate privacy and legal (regulatory) limitations.
* Recognize and describe major industry players, including leading vendors and cloud service providers.
* List and explain technical and behavioral parameters used to characterize language models (LLMs and SLMs).
* List the most common LLM benchmarks.
* List the main approaches for expanding LLM capabilities.
* Classify the LLM plugins and give examples for each plugin type.
* Describe the structure and workflow of an RAG system.
* Recall cases in different industries where AI was successfully implemented.


# AI Asisted Engineering
* 